# -*- coding: utf-8 -*-
"""CS 5350 Project Report

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jlDanCIQKnD2AHhXjb2IZd7chA5318eB

#Workflow / Strategy:
1.   Load the data and preprocess it.
2.   Train a new baseline model or learner.
3.   Evaluate model or learner by submitting test data predictions.
4.   Identify improvement areas.
5.   Repeat steps one through five until satisfied.

# Create Test Submission for Kaggle
"""

# Import Packages
import random
import pandas as pd

# Create a list of ID values
id_values = list(range(1, 23843))

# Create a list of Prediction values
prediction_values = [random.randint(0, 1) for i in range(23842)]

# Create a Pandas DataFrame
df = pd.DataFrame({'ID': id_values, 'Prediction': prediction_values})

# Download the CSV file
df.to_csv('predictions.csv', index=False)

"""# Preprocess Data

###Find Missing Values
"""

# Import Packages
from sklearn.impute import SimpleImputer
import textblob

def findMissingValues(train, test):
  # Load the datasets into Pandas DataFrames.
  train_df = pd.read_csv(train)
  test_df = pd.read_csv(test)

  # Identify missing values in the train DataFrame
  train_df_missing = train_df.isnull().sum()

  # Identify missing values in the test DataFrame
  test_df_missing = test_df.isnull().sum()

  # Print the missing values for each attribute
  print(train_df_missing)
  print(test_df_missing)
  train_df.info()
  test_df.info()

findMissingValues('train_final.csv', 'test_final.csv')

"""###Find '?' Values"""

def findQuestionValues(train, test):
  # Load the datasets into Pandas DataFrames.
  train_df = pd.read_csv(train)
  test_df = pd.read_csv(test)
  # Find rows with '?' values in the train DataFrame
  train_df_with_question_marks = train_df.loc[train_df.eq('?').any(axis=1)]
  train_df_question_mark_counts = train_df.eq('?').sum()

  # Find rows with '?' values in the test DataFrame
  test_df_with_question_marks = test_df.loc[test_df.eq('?').any(axis=1)]
  test_df_question_mark_counts = test_df.eq('?').sum()

  # Print the count of '?' values for each column in the train DataFrame
  print('Train DataFrame:')
  print(train_df_question_mark_counts)

  # Print the count of '?' values for each column in the test DataFrame
  print('Test DataFrame:')
  print(test_df_question_mark_counts)

findQuestionValues('train_final.csv', 'test_final.csv')

"""###Find Incorrectly Entered Values / Outliers"""

# Load data.
train_df = pd.read_csv('train_final.csv')
test_df = pd.read_csv('test_final.csv')

# Describe training data.
train_df.describe()

# Describe testing data.
test_df.describe()

# Find Any Incorrectly Entered Values:
# Report all value options for each categorical column
# Report all instances of non-numerical values in numerical columns

# Split into numerical and categorical columns.
train_categorical_columns = train_df.select_dtypes(include=['object', 'category']).columns
train_numerical_columns = train_df.select_dtypes(include=['number']).columns

  # Split into numerical and categorical columns.
test_categorical_columns = test_df.select_dtypes(include=['object', 'category']).columns
test_numerical_columns = test_df.select_dtypes(include=['number']).columns

def findIncorrectValues(train, test):
  # Load the datasets into Pandas DataFrames.
  train_df = pd.read_csv(train)
  test_df = pd.read_csv(test)

  # Identify datatypes for both datasets:
  # Identify datatypes.
  print("Test Data Types:")
  print(test_df.dtypes)
  print()
  print("Training Data Types:")
  print(train_df.dtypes)
  print()


  # For Training Data: ------------
  # Identify datatypes.
  print("Training data Value Counts ---------")

  # Create a dictionary to store the categorical values and their count per value
  train_categorical_value_counts = {}

  for column in train_categorical_columns:
      # Get the unique values in the column
      unique_values = train_df[column].unique()

      # Count the number of times each value occurs in the column
      value_counts = train_df[column].value_counts()

      # Add the value counts to the dictionary
      train_categorical_value_counts[column] = value_counts

  # Print the categorical values and their count per value
  print('Categorical Values:')
  for column, value_counts in train_categorical_value_counts.items():
      print(f'{column}: {value_counts}')

  # For Test Data: ------------
  print()
  print("Testing data Value Counts ---------")

  # Create a dictionary to store the categorical values and their count per value
  test_categorical_value_counts = {}

  for column in test_categorical_columns:
      # Get the unique values in the column
      unique_values = test_df[column].unique()

      # Count the number of times each value occurs in the column
      value_counts = test_df[column].value_counts()

      # Add the value counts to the dictionary
      test_categorical_value_counts[column] = value_counts

  # Print the categorical values and their count per value
  print('Categorical Values:')
  for column, value_counts in test_categorical_value_counts.items():
      print(f'{column}: {value_counts}')

  # -------------
  print()
  # Create a list to store the non-numerical entries in the numerical columns that are not '?'
  train_non_numerical_entries = []
  test_non_numerical_entries = []

  for column in train_numerical_columns:
      # Check if each value in the column is a number and is not '?'
      for value in train_df[column]:
          if not isinstance(value, (int, float)) and value != '?':
              # If the value is not a number and is not equal to '?', add it to the list of non-numerical entries
              train_non_numerical_entries.append((column, value))

  for column in test_numerical_columns:
      for value in test_df[column]:
          if not isinstance(value, (int, float)) and value != '?':
              test_non_numerical_entries.append((column, value))

  # Print the non-numerical entries in the numerical columns that are not '?'
  if train_non_numerical_entries:
      print('Non-Numerical Entries in Training:')
      for column, value in train_non_numerical_entries:
          print(f'{column}: {value}')
  else:
      print('There are no non-numerical entries in the numerical columns that are not ? in training data.')

  if test_non_numerical_entries:
      print('Non-Numerical Entries in Testing:')
      for column, value in test_non_numerical_entries:
          print(f'{column}: {value}')
  else:
      print('There are no non-numerical entries in the numerical columns that are not ? in test data.')

findIncorrectValues('train_final.csv', 'test_final.csv')

# Find numerical outliers using IQR.

def calculate_iqr_outliers(df, column):
    """
    Calculates the interquartile range (IQR) for the given column in the given dataframe and identifies outliers.

    Args:
        df: A Pandas dataframe.
        column: The name of the column to calculate the IQR for.

    Returns:
        A list of tuples containing the row number and value of each outlier.
    """

    # Calculate the IQR
    q1 = df[column].quantile(0.25)
    q3 = df[column].quantile(0.75)
    iqr = q3 - q1

    # Calculate the lower and upper bounds for outliers
    lower_bound = q1 - (1.5 * iqr)
    upper_bound = q3 + (1.5 * iqr)

    # Identify outliers
    outliers = []
    for i in range(len(df)):
        value = df[column].iloc[i]
        if value < lower_bound or value > upper_bound:
            outliers.append((i, value))

    return outliers
# Calculate the IQR outliers for the train dataframe
train_df_outliers = {}
for column in train_numerical_columns:
    train_df_outliers[column] = calculate_iqr_outliers(train_df, column)

# Calculate the IQR outliers for the test dataframe
test_df_outliers = {}
for column in test_numerical_columns:
    test_df_outliers[column] = calculate_iqr_outliers(test_df, column)

"""###Impute Values to Create Clean Datasets"""

# Imports
from sklearn.impute import SimpleImputer
import numpy as np

# Create SimpleImputer for each type of column and dataset.
train_cat_imputer = SimpleImputer(strategy='most_frequent', fill_value='?')
train_num_imputer = SimpleImputer(strategy='mean', fill_value='?')
test_cat_imputer = SimpleImputer(strategy='most_frequent', fill_value='?')
test_num_imputer = SimpleImputer(strategy='mean', fill_value='?')

# Remove '?' values so imputer can impute them
train_df[train_categorical_columns] = train_df[train_categorical_columns].replace('?', np.nan)
train_df[train_numerical_columns] = train_df[train_numerical_columns].replace('?', np.nan)
test_df[test_categorical_columns] = test_df[test_categorical_columns].replace('?', np.nan)
test_df[test_numerical_columns] = test_df[test_numerical_columns].replace('?', np.nan)

# Fit imputers to training data
train_cat_imputer.fit(train_df[train_categorical_columns])
train_num_imputer.fit(train_df[train_numerical_columns])
test_cat_imputer.fit(test_df[test_categorical_columns])
test_num_imputer.fit(test_df[test_numerical_columns])

# Impute categorical missing values
train_df[train_categorical_columns] = train_cat_imputer.transform(train_df[train_categorical_columns])
test_df[test_categorical_columns] = test_cat_imputer.transform(test_df[test_categorical_columns])

# Impute numerical missing values
train_df[train_numerical_columns] = train_num_imputer.transform(train_df[train_numerical_columns])
test_df[test_numerical_columns] = test_num_imputer.transform(test_df[test_numerical_columns])

# Drop entries in test data not present in training data:
def replace_entry_of_holand_netherlands_with_most_common_country(test_df):
  """Replaces the entry of "Holand-Netherlands" in the test dataset with the most common country.

  Args:
    test_df: A pandas DataFrame containing the test data.

  Returns:
    A pandas DataFrame containing the test data with the entry of "Holand-Netherlands" replaced with the most common country.
  """

  # Find the most common country in the test dataset.
  most_common_country = test_df['native.country'].mode()[0]

  # Replace the entry of "Holand-Netherlands" in the test dataset with the most common country.
  test_df.loc[test_df['native.country'] == 'Holand-Netherlands', 'native.country'] = most_common_country

  return test_df

# Replace the entry of "Holand-Netherlands" in the test dataset with the most common country.
test_df = replace_entry_of_holand_netherlands_with_most_common_country(test_df)

# Save csv as copy of data
train_df.to_csv('train_imputed.csv', index=False)
test_df.to_csv('test_imputed.csv', index=False)

# Verify that imputation worked
findMissingValues('train_imputed.csv', 'test_imputed.csv')
findQuestionValues('train_imputed.csv', 'test_imputed.csv')
findIncorrectValues('train_imputed.csv', 'test_imputed.csv')

"""### Data Visualizations

#Train Learner / Baseline Model

## Initializing

### Import and Conversion
"""

# Import Preprocessed Data.
train_df = pd.read_csv('train_imputed.csv')
test_df = pd.read_csv('test_imputed.csv')

# Convert Categorical Variables to Dummy -------

# Categorical columns for both datasets
train_categorical_cols = [col for col in train_df.columns if train_df[col].dtype == 'object']
test_categorical_cols = [col for col in train_df.columns if train_df[col].dtype == 'object']

# Get dummies for the categorical variables in the train_df dataset.
train_df = pd.get_dummies(train_df, columns=train_categorical_cols)

# Get dummies for the categorical variables in the test_df dataset.
test_df = pd.get_dummies(test_df, columns=test_categorical_cols)

# Save the encoded train_df and test_df datasets.
train_df.to_csv('train_encoded.csv', index=False)
test_df.to_csv('test_encoded.csv', index=False)

# --------

"""## Using Logistic Regression

### Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# Create Logistic Regression Object
clf = LogisticRegression()

# Fit Model to Training Data.
clf.fit(train_df[train_df.columns.drop('income>50K')], train_df['income>50K'])

# Make predictions on the Test Data.
test_predictions = clf.predict_proba(test_df[test_df.columns.drop('ID')])[:, 1]

# Round every prediction to either 0 or 1.
test_predictions = np.round(test_predictions)

# Save predictions to CSV
test_predictions_df = pd.DataFrame({'ID': test_df['ID'], 'Prediction': test_predictions})
test_predictions_df.to_csv('predictions_LR.csv', index=False)

"""## Using Neural Network"""

# Imports
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Separate features and labels
X_train = train_df.drop("income>50K", axis=1)
y_train = train_df["income>50K"]
X_test = test_df.drop("ID", axis=1)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Build the neural network
model = Sequential()
model.add(Dense(256, activation="relu", input_shape=(X_train_scaled.shape[1],)))
model.add(Dense(128, activation="relu"))
model.add(Dense(1, activation="sigmoid"))
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

# Train the model
model.fit(X_train_scaled, y_train, epochs=20, batch_size=35, validation_split=0.2)

# Make predictions on test data
y_pred = model.predict(X_test_scaled)
y_pred_binary = y_pred > 0.5

# Save predictions
test_df["income"] = y_pred_binary.astype(int)
test_df.to_csv("predicted_income.csv", index=False)

# Create a column with row indices as IDs
test_df["ID"] = pd.Series(range(1, len(test_df) + 1))

# Select just the "ID" and "Prediction" columns
predictions_df = test_df[["ID", "income"]]

# Save the "predictions_df" dataframe as a CSV file
predictions_df.to_csv("NN_predictions9.csv", index=False)

print("Prediction CSV saved")

"""# Improvement Log

### Scores:

*   Weighted Random Guessing - 0.4984
* Random Guessing - 0.5037
* Logistic Regression with non binary predictions - 0.57717
* Logistic regression with binary predictions - 0.6124
* Neural Network 3-Layer with Relu, and Adam - 0.75279



"""